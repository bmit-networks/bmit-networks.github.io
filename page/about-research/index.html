<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
  <head>
    

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

 


      <title>Research - BMIT Research Group</title>

  <meta name="description" content="Our Research To update"><script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "WebSite",
    "name": "BMIT Reasearch Group",
    
    "url": "https:\/\/bmit-networks.github.io\/"
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "",
  "url": "https:\/\/bmit-networks.github.io\/"
  
  
  
  
}
</script>
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
        "@type": "ListItem",
        "position": 1,
        "item": {
          "@id": "https:\/\/bmit-networks.github.io\/",
          "name": "home"
        }
    },{
        "@type": "ListItem",
        "position": 3,
        "item": {
          "@id": "https:\/\/bmit-networks.github.io\/page\/about-research\/",
          "name": "Research"
        }
    }]
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "author": {
    "name" : ""
  },
  "headline": "Research",
  "description" : "Our Research To update",
  "inLanguage" : "en",
  "wordCount":  4 ,
  "datePublished" : "0001-01-01T00:00:00",
  "dateModified" : "0001-01-01T00:00:00",
  "image" : "https:\/\/bmit-networks.github.io\/",
  "keywords" : [ "" ],
  "mainEntityOfPage" : "https:\/\/bmit-networks.github.io\/page\/about-research\/",
  "publisher" : {
    "@type": "Organization",
    "name" : "https:\/\/bmit-networks.github.io\/",
    "logo" : {
        "@type" : "ImageObject",
        "url" : "https:\/\/bmit-networks.github.io\/",
        "height" :  60 ,
        "width" :  60
    }
  }
}
</script>

<meta property="og:title" content="Research" />
<meta property="og:description" content="Our Research To update">
<meta property="og:url" content="https://bmit-networks.github.io/page/about-research/" />
<meta property="og:type" content="website" />
<meta property="og:site_name" content="BMIT Reasearch Group" />

  <meta name="twitter:title" content="Research" />
  <meta name="twitter:description" content="Our Research To update">
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="generator" content="Hugo 0.124.1">
  <link rel="alternate" href="https://bmit-networks.github.io/index.xml" type="application/rss+xml" title="BMIT Reasearch Group"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css" integrity="sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/css/bootstrap.min.css" integrity="sha384-HSMxcRTRxnN+Bdg0JdbxYKrThecOKuH5zCYotlSAcp1+c8xmyTe9GYg1l9a69psu" crossorigin="anonymous"><link rel="stylesheet" href="https://bmit-networks.github.io/css/main.css" /><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" />
  <link rel="stylesheet" href="https://bmit-networks.github.io/css/highlight.min.css" /><link rel="stylesheet" href="https://bmit-networks.github.io/css/codeblock.css" /><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css" integrity="sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css" integrity="sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R" crossorigin="anonymous">

  </head>
  <body>
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="https://bmit-networks.github.io/">BMIT Reasearch Group</a>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
        
          
            <li class="navlinks-container">
              <a class="navlinks-parent" role="button" tabindex="0">About</a>
              <div class="navlinks-children">
                
                  <a href="/page/about">About Us</a>
                
                  <a href="/page/about-people">People</a>
                
                  <a href="/page/about-research">Our Research</a>
                
                  <a href="/page/about-news">News</a>
                
              </div>
            </li>
          
        
          
            <li>
              <a title="Opportunities" href="/page/opportunity">Opportunities</a>
            </li>
          
        
          
            <li>
              <a title="SJTU-USYD Alliance" href="/page/stju-usyd-alliance">SJTU-USYD Alliance</a>
            </li>
          
        
          
            <li>
              <a title="BMIT-GRN" href="/page/bmit-grn">BMIT-GRN</a>
            </li>
          
        

        

        
      </ul>
    </div>

    

  </div>
</nav>




    


<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>


  
  
  






  

  <header class="header-section ">
    
    
    <div class="intro-header no-img">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <div class="page-heading">
              
                <h1>Research</h1>
              
              
                <hr class="small">
              
              
              
            </div>
          </div>
        </div>
      </div>
    </div>
  
  </header>


    
<div class="container" role="main">
  <div class="row">
    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
      <article role="main" class="blog-post">
      
    
        <h1 id="our-research">Research Projects</h1>
<p> </p>        
<br>


<p><div class="splitbox"><div class="left">
<img src="/img/RP-1-3d-point-cloud-tom.png" alt="">
</div><div class="right">

<h4>3D Point Cloud Processing and Analysis</h4>

<p style="font-size:0.91em"><b>Project Leader: 
<a href="https://www.sydney.edu.au/engineering/about/our-people/academic-staff/tom-cai.html" TARGET="_blank">A/Prof. Weidong (Tom) Cai</a></b></p>
<p style="font-size:0.8em">
    Point cloud is a principal data structure for 3D geometric representation learning. 
    Unlike other conventional visual data, such as images and videos, these irregular points 
    describe the complex shape features of 3D objects or scenes, which are intrinsically close 
    to the essence of 3D computer vision. With recent progress in deep learning, from micro to 
    macro, a growing interest has been rapidly accumulated within the community to promote and 
    facilitate a line of cutting-edge research studies, including (1) designing advanced point 
    cloud processing frameworks for effective 3D object classification and segmentation, (2) 
    establishing more comprehensive object-centric understandings for efficient 3D point cloud 
    analysis, and, (3) shedding lights on a bunch of creative AI-enlightened indoor/outdoor 
    scene-oriented applications such as interior decoration designing and self-driving automobile. 
    Our primary research interests include: 3D point-based scene understanding; Point-wise 
    information propagation and processing; Learning rotation-invariance in point cloud data; 
    Medical point cloud analysis for disease detection and treatment; Audio-visual navigation 
    in complex 3D environments for multimedia computing.
</p>

</div><div style="clear:both"></div></div>
</p>
<br>
<br>

<p><div class="splitbox"><div class="left">
<img src="/img/RP-2-biomedical-m-r-v-jinman.jpg" alt="">
</div><div class="right">

<h4>Biomedical Mixed Reality Visualisation</h4>

<p style="font-size:0.91em"><b>Project Leader: 
<a href="https://www.sydney.edu.au/engineering/about/our-people/academic-staff/jinman-kim.html" TARGET="_blank">Prof. Jinman Kim</a></b></p>
<p style="font-size:0.8em">
    The next generation of medical imaging scanners are introducing new diagnostic capabilities 
    that improve patient care. These medical images are multi-dimensional (3D), multi-modality 
    (fusion of PET and MRI for example) and also time varying (that is, 3D volumes taken over 
    multiple time points and functional MRI). Our research innovates in coupling visualization 
    technologies, including hardware innovation such as Mixed Reality headsets, with AI algorithms 
    to render realistic and detailed 3D representation of the body. Our recent works include generating 
    medical image volume rendering from single image slices and RIbMR that uses HoloLens2 to augment 
    fractured rib bones to the patient’s body during surgery.
</p>

</div><div style="clear:both"></div></div>
</p>
<br>
<br>

<p><div class="splitbox"><div class="left">
<img src="/img/RP-3-cognition-inspired-ogi-xiuying.png" alt="">
</div><div class="right">

<h4>Cognition-Inspired Ontology-Guided Image Content Understanding</h4>

<p style="font-size:0.91em"><b>Project Leader: 
<a href="https://www.sydney.edu.au/engineering/about/our-people/academic-staff/xiu-wang.html" TARGET="_blank">A/Prof. Xiuying Wang</a></b></p>
<p style="font-size:0.8em">
    While we have witnessed rapid development and wide use of DNN in different areas, by far, 
    human cognition is yet to be properly incorporated for deep learning model design. We focus 
    on innovative design of deep learning models to resemble human cognition for saliency detection, 
    the initial research on mimicking human cognitive thinking of images and our approach outperformed 
    17 benchmarking DNN methods on six well-recognized datasets. To further this cognition-inspired research, 
    our current research is focusing on ontology-guided scene graph generation for image content understanding. 
    In our research, ontology is extracted from Large Language Models (LLMs) and fuse with imagens from image 
    representation learning. This ongoing research effort is to innovate image content understanding for 
    diverse applications.
</p>

</div><div style="clear:both"></div></div>
</p>
<br>
<br>

<p><div class="splitbox"><div class="left">
<img src="/img/RP-4-cross-domain-mia-tom.png" alt="">
</div><div class="right">

<h4>Cross-domain Medical Image Analysis and Processing</h4>

<p style="font-size:0.91em"><b>Project Leader: 
<a href="https://www.sydney.edu.au/engineering/about/our-people/academic-staff/tom-cai.html" TARGET="_blank">A/Prof. Weidong (Tom) Cai</a></b></p>
<p style="font-size:0.8em">
    Medical image analysis / processing is an essential step for AI-enhanced diseases diagnosis. 
    With the success of deep learning, recent deep neural network-based methods are now prevalent 
    in medical image segmentation and bio-anomaly detection. Although these fully supervised methods 
    achieve state-of-the-art performance, their high accuracy heavily relies on massively annotated 
    training images from the specific domains. When tested these off-the-shelf models on the images 
    from new unseen domains, the performance suffers from a significant drop. On the other hand, 
    it is impractical to acquire sufficient annotations for each new data source, since the labelling 
    process for computer vision tasks on complex medical imaging data is time-consuming, labour-intensive, 
    and error-prone. This project aims to study enhancing the generalization abilities of current AI models, 
    and to develop novel frameworks for cross-domain medical image analysis and processing.
</p>

</div><div style="clear:both"></div></div>
</p>
<br>
<br>

<p><div class="splitbox"><div class="left">
<img src="/img/RP-5-human-behavior-u-zhiyong.png" alt="">
</div><div class="right">

<h4>Human Behavior Understanding</h4>

<p style="font-size:0.91em"><b>Project Leader: 
<a href="https://www.sydney.edu.au/engineering/about/our-people/academic-staff/zhiyong-wang.html" TARGET="_blank">Prof. Zhiyong Wang</a></b></p>
<p style="font-size:0.8em">
    Understanding each individual in our society is a fundamental social function for human beings. 
    This project aims to develop cutting edge deep learning techniques for understanding human behavior 
    in both physical and digital world, such as human action understanding, affective state analysis, 
    social interactions in social media platforms, and purchase activities in eCommerce platforms. 
    Expected outcomes of this project should provide significant benefits to a wide range of domains, 
    such as health, sports, social media, and digital retailer.
</p>

</div><div style="clear:both"></div></div>
</p>
<br>
<br>

<p><div class="splitbox"><div class="left">
<img src="/img/RP-6-multimedia-c-c-zhiyong.png" alt="">
</div><div class="right">

<h4>Multimedia Content Creation</h4>

<p style="font-size:0.91em"><b>Project Leader: 
<a href="https://www.sydney.edu.au/engineering/about/our-people/academic-staff/zhiyong-wang.html" TARGET="_blank">Prof. Zhiyong Wang</a></b></p>
<p style="font-size:0.8em">
    Recent success of deep learning has demonstrated a great potential to create media content, 
    such as generating human motion. This has opened a new door for creativity and innovation 
    in many domains, such as media, film, and game, even metaverse. This project aims to address 
    the technical challenges of creating highly realistic media content by developing novel 
    computing techniques, such as audio/image/video generation and editing, motion retargeting, 
    3D animation, cross-modal simulation, and 3D physical simulation. Students will gain comprehensive 
    knowledge in multimedia data processing, computer vision, 3D vision, computer graphics, and machine learning.
</p>

</div><div style="clear:both"></div></div>
</p>
<br>
<br>

<p><div class="splitbox"><div class="left">
<img src="/img/RP-7-multi-modal-cross-modal-jinman.png" alt="">
</div><div class="right">

<h4>Multi-modal and Cross-modal Learning for Biomedical Image-omics</h4>

<p style="font-size:0.91em"><b>Project Leader: 
<a href="https://www.sydney.edu.au/engineering/about/our-people/academic-staff/jinman-kim.html" TARGET="_blank">Prof. Jinman Kim</a></b></p>
<p style="font-size:0.8em">
    Clinical imaging is ubiquitous in modern medicine for the evaluation of serious medical 
    conditions. These methods are often used in isolation, despite the rich complementary data 
    sources – such as biological assays – that typically accompany imaging. This project aims 
    to develop an automated machine learning pipeline that augments primary imaging data with 
    a diverse range of existing and potential supporting data sources and collectively co-learns 
    to enable optimal diagnoses and treatment options, and new insights into disease mechanisms. 
    Recent projects include radiogenomics, vision-language pretraining (VLP) and explainable AI 
    for biomedical data.
</p>

</div><div style="clear:both"></div></div>
</p> 
<br>
<br>
        
<p><div class="splitbox"><div class="left">
<img src="/img/RP-8-multi-modal-image-centric-xiuying.png" alt="">
</div><div class="right">

<h4>Multi-modal Image-centric Computing and Spatial-Feature Fusion for Medical Applications</h4>

<p style="font-size:0.91em"><b>Project Leader: 
<a href="https://www.sydney.edu.au/engineering/about/our-people/academic-staff/xiu-wang.html" TARGET="_blank">A/Prof. Xiuying Wang</a></b></p>
<p style="font-size:0.8em">
    Multi-modal, multi-factorial data are becoming indispensable for modern precision medicine.  
    We endeavour to develop synergic fusion approaches to harness multi-modal image computing and 
    multi-factorial analytics for assisting medical applications. Our computer vision methods 
    provide solutions to critical questions raised by high dimensionality and multimodal complexity. 
    Based on our computational algorithms and models, we have fused important imaging features 
    with domain factors including genomic information, biological factors for classification and 
    prediction applications. We put emphasis on interpreting the process of decision making from 
    machine learning models for practical applications. We developed graph models to represent 
    the relational structure and geometric and topological patterns in interpretable learning 
    process and for prognostic analysis. Our research focuses on dynamic topological pattern 
    recognition through incorporating both graph structure and feature information with updating status.
</p>

</div><div style="clear:both"></div></div>
</p>

        

        

        

        
      </article>

      


      

    </div>
  </div>
</div>

      
    <div class="page-meta">
  
  
  
</div>


  
<footer>
  <div class="container">
    
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
          
        </ul>
        <p class="credits copyright text-muted">
          

          &nbsp;&bull;&nbsp;&copy;
          
            2024
          

          
            &nbsp;&bull;&nbsp;
            <a href="https://bmit-networks.github.io/">BMIT Reasearch Group</a>
          
        </p>
        
        <p class="credits theme-by text-muted">
          <a href="https://gohugo.io">Hugo v0.124.1</a> powered &nbsp;&bull;&nbsp; Theme <a href="https://github.com/halogenica/beautifulhugo">Beautiful Hugo</a> adapted from <a href="https://deanattali.com/beautiful-jekyll/">Beautiful Jekyll</a>
          
        </p>
      </div>
    </div>
  </div>
</footer><script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js" integrity="sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<script src="https://code.jquery.com/jquery-3.7.0.slim.min.js" integrity="sha384-w5y/xIeYixWvfM+A1cEbmHPURnvyqmVg5eVENruEdDjcyRLUSNej7512JQGspFUr" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/js/bootstrap.min.js" integrity="sha384-aJ21OjlMXNL5UyIl/XNwTMqvzeRMZH2w8c5cRVpzpU8Y5bApTppSuUkhZXN0VxHd" crossorigin="anonymous"></script>

<script src="https://bmit-networks.github.io/js/main.js"></script>
<script src="https://bmit-networks.github.io/js/highlight.min.js"></script>
<script> hljs.initHighlightingOnLoad(); </script>
<script> $(document).ready(function() {$("pre.chroma").css("padding","0");}); </script><script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js" integrity="sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js" integrity="sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q" crossorigin="anonymous"></script><script src="https://bmit-networks.github.io/js/load-photoswipe.js"></script>









    
  </body>
</html>

